{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import translate_v2 as translate\n",
    "import os\n",
    "from google_auth_oauthlib import flow\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    text = \"Hello world\"\n",
    "    result = translate_client.translate(text, target_language=\"mk\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_prepare_data(path):\n",
    "    with open(path) as f:\n",
    "        lines = f.read().splitlines()\n",
    "        df_inter = pd.DataFrame(lines)\n",
    "        df_inter.columns = ['json_element']\n",
    "        df_inter['json_element'].apply(json.loads)\n",
    "        df_final = pd.json_normalize(df_inter['json_element'].apply(json.loads))\n",
    "        \n",
    "        return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multirc_preprocessing(original_dataset_dest):\n",
    "    df_final = read_and_prepare_data(original_dataset_dest)\n",
    "\n",
    "    multi_rc = {}\n",
    "    for index, row in tqdm(df_final.iterrows()):\n",
    "    #     print(pd.json_normalize(row['passage.questions']))\n",
    "        multi_rc[index] = {}\n",
    "        multi_rc[index][\"idx\"] = row[\"idx\"]\n",
    "        multi_rc[index][\"passage_text\"] = row[\"passage.text\"]\n",
    "        multi_rc[index][\"questions\"] = row[\"passage.questions\"]\n",
    "        \n",
    "    return multi_rc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_client = translate.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"translation_api_credentials.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = [\"train\", \"test\", \"val\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MULTIRC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtype in dtypes:\n",
    "    print(dtype)\n",
    "    \n",
    "    original_dataset_dest = f\"MultiRC/{dtype}.jsonl\"\n",
    "    translated_dataset_dest = f\"New/multirc_{dtype}_mk.json\"\n",
    "    multi_rc = multirc_preprocessing(original_dataset_dest)\n",
    "    multi_rc_dict = {}\n",
    "    \n",
    "    for index, row in tqdm(multi_rc.items()):\n",
    "\n",
    "        multi_rc_dict[index] = {}\n",
    "\n",
    "        translated_passage = translate_client.translate(\n",
    "            row['passage_text'], target_language='mk')[\"translatedText\"]\n",
    "        multi_rc_dict[index]['passage'] = translated_passage\n",
    "\n",
    "        questions = row['questions']\n",
    "        translated_questions_lst = []\n",
    "        for question in questions:\n",
    "            # Question is a dictionary\n",
    "            current_question_dict = {}\n",
    "            current_question_dict['idx'] = question['idx']\n",
    "            q = question['question']\n",
    "            current_question_dict['question'] = translate_client.translate(\n",
    "                q, target_language='mk')[\"translatedText\"]\n",
    "            answers = question['answers']\n",
    "            current_answers = []\n",
    "            for ans_id, ans in enumerate(answers):\n",
    "                # Ans is a dictionary\n",
    "                answer_translation = {}\n",
    "                answer_translation['idx'] = ans['idx']\n",
    "\n",
    "                if dtype!=\"test\":\n",
    "                    answer_translation['label'] = ans['label']\n",
    "\n",
    "                answer_translation['text'] = ''\n",
    "    #             if len(ans['text']) > 0 and not(urlparse(ans['text']).scheme and urlparse(ans['text']).netloc):\n",
    "                try:\n",
    "                    answer_translation['text'] = translate_client.translate(\n",
    "                        ans['text'], target_language='mk')[\"translatedText\"]\n",
    "                except:\n",
    "                    answer_translation['text'] = ans['text']\n",
    "\n",
    "                current_answers.append(answer_translation)\n",
    "\n",
    "    #             time.sleep(0.1)\n",
    "\n",
    "    #         time.sleep(1)\n",
    "\n",
    "            current_question_dict['answers'] = current_answers\n",
    "            translated_questions_lst.append(current_question_dict)\n",
    "\n",
    "        multi_rc_dict[index]['questions'] = translated_questions_lst\n",
    "\n",
    "\n",
    "    #     if index % 50 == 0:\n",
    "        with open(f\"{translated_dataset_dest}\", \"w\") as outfile:\n",
    "            json.dump(multi_rc_dict, outfile, ensure_ascii=False)\n",
    "\n",
    "    #     time.sleep(0.1)\n",
    "    with open(f\"{translated_dataset_dest}\", \"w\") as outfile:\n",
    "        json.dump(multi_rc_dict, outfile, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. BOOLQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtype in dtypes:\n",
    "    print(dtype)\n",
    "    original_dataset_dest = f\"BoolQ/{dtype}.jsonl\"\n",
    "    translated_dataset_dest = f\"New/boolq_{dtype}_mk.json\"\n",
    "    df_final = read_and_prepare_data(original_dataset_dest)\n",
    "    \n",
    "    boolq_dict = {}\n",
    "    for index, row in tqdm(df_final.iterrows()):\n",
    "    \n",
    "        boolq_dict[index] = {}\n",
    "        translated_question = translate_client.translate(row['question'], target_language='mk')[\"translatedText\"]\n",
    "        translated_passage = translate_client.translate(row['passage'], target_language='mk')[\"translatedText\"]\n",
    "\n",
    "        boolq_dict[index]['question'] = translated_question\n",
    "        boolq_dict[index]['passage'] = translated_passage\n",
    "\n",
    "        if dtype != \"test\":\n",
    "            boolq_dict[index]['label'] = row['label']\n",
    "\n",
    "        if index % 500 == 0:\n",
    "            with open(translated_dataset_dest, \"w\") as outfile:\n",
    "                json.dump(boolq_dict, outfile, ensure_ascii=False)\n",
    "\n",
    "    #     time.sleep(0.5)\n",
    "    with open(translated_dataset_dest, \"w\") as outfile:\n",
    "        json.dump(boolq_dict, outfile, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. COPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtype in dtypes:\n",
    "    print(dtype)\n",
    "    original_dataset_dest = f\"COPA/{dtype}.jsonl\"\n",
    "    translated_dataset_dest = f\"New/copa_{dtype}_mk.json\"\n",
    "    df_final = read_and_prepare_data(original_dataset_dest)\n",
    "    \n",
    "    copa_dict = {}\n",
    "    for index, row in tqdm(df_final.iterrows()):\n",
    "\n",
    "        copa_dict[index] = {}\n",
    "    #     translated_question = translate_client.translate(row['question'], target_language='mk')[\"translatedText\"]\n",
    "        translated_premise = translate_client.translate(row['premise'], target_language='mk')[\"translatedText\"]\n",
    "        translated_choice1 = translate_client.translate(row['choice1'], target_language='mk')[\"translatedText\"]\n",
    "        translated_choice2 = translate_client.translate(row['choice2'], target_language='mk')[\"translatedText\"]\n",
    "\n",
    "        copa_dict[index]['question'] = row['question']\n",
    "        copa_dict[index]['premise'] = translated_premise\n",
    "        copa_dict[index]['choice1'] = translated_choice1\n",
    "        copa_dict[index]['choice2'] = translated_choice2\n",
    "\n",
    "        if dtype != \"test\":\n",
    "            copa_dict[index]['label'] = row['label']\n",
    "\n",
    "        if index % 500 == 0:\n",
    "            with open(translated_dataset_dest, \"w\") as outfile:\n",
    "                json.dump(copa_dict, outfile, ensure_ascii=False)\n",
    "\n",
    "    time.sleep(0.5)\n",
    "    with open(translated_dataset_dest, \"w\") as outfile:\n",
    "        json.dump(copa_dict, outfile, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtype in dtypes:\n",
    "    print(dtype)\n",
    "    original_dataset_dest = f\"RTE/{dtype}.jsonl\"\n",
    "    translated_dataset_dest = f\"New/rte_{dtype}_mk.json\"\n",
    "    df_final = read_and_prepare_data(original_dataset_dest)\n",
    "    \n",
    "    current_dict = {}\n",
    "    \n",
    "    for index, row in tqdm(df_final.iterrows()):\n",
    "\n",
    "    \n",
    "        current_dict[index] = {}\n",
    "        translated_premise = translate_client.translate(row['premise'], target_language='mk')[\"translatedText\"]\n",
    "        translated_hypothesis = translate_client.translate(row['hypothesis'], target_language='mk')[\"translatedText\"]\n",
    "\n",
    "        current_dict[index]['hypothesis'] = translated_hypothesis\n",
    "        current_dict[index]['premise'] = translated_premise\n",
    "\n",
    "        if dtype != \"test\":\n",
    "            current_dict[index]['label'] = row['label']\n",
    "\n",
    "        if index % 500 == 0:\n",
    "            with open(translated_dataset_dest, \"w\") as outfile:\n",
    "                json.dump(current_dict, outfile, ensure_ascii=False)\n",
    "    \n",
    "    with open(translated_dataset_dest, \"w\") as outfile:\n",
    "        json.dump(current_dict, outfile, ensure_ascii=False)\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. WIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtype in dtypes:\n",
    "    print(dtype)\n",
    "    original_dataset_dest = f\"WiC/{dtype}.jsonl\"\n",
    "    translated_dataset_dest = f\"New/wic_{dtype}_mk.json\"\n",
    "    \n",
    "    df_final = read_and_prepare_data(original_dataset_dest)\n",
    "    \n",
    "    current_dict = {}\n",
    "    \n",
    "    for index, row in tqdm(df_final.iterrows()):\n",
    "\n",
    "    \n",
    "        current_dict[index] = {}\n",
    "        translated_word = translate_client.translate(row['word'], target_language='mk')[\"translatedText\"]\n",
    "        translated_sentence1 = translate_client.translate(row['sentence1'], target_language='mk')[\"translatedText\"]\n",
    "        translated_sentence2 = translate_client.translate(row['sentence2'], target_language='mk')[\"translatedText\"]\n",
    "\n",
    "        current_dict[index]['word'] = translated_word\n",
    "        current_dict[index]['sentence1'] = translated_sentence1\n",
    "        current_dict[index]['sentence2'] = translated_sentence2\n",
    "        current_dict[index]['start1'] = row['start1']\n",
    "        current_dict[index]['start2'] = row['start2']\n",
    "        current_dict[index]['end1'] = row['end1']\n",
    "        current_dict[index]['end2'] = row['end2']\n",
    "        \n",
    "\n",
    "        if dtype != \"test\":\n",
    "            current_dict[index]['label'] = row['label']\n",
    "\n",
    "        if index % 500 == 0:\n",
    "            with open(translated_dataset_dest, \"w\") as outfile:\n",
    "                json.dump(current_dict, outfile, ensure_ascii=False)\n",
    "    \n",
    "    with open(translated_dataset_dest, \"w\") as outfile:\n",
    "        json.dump(current_dict, outfile, ensure_ascii=False)\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. AX-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset_dest = f\"AX-b/AX-b.jsonl\"\n",
    "df_final = read_and_prepare_data(original_dataset_dest)\n",
    "translated_dataset_dest = f\"New/ax-b_mk.json\"\n",
    "current_dict = {}\n",
    "for index, row in tqdm(df_final.iterrows()):\n",
    "\n",
    "\n",
    "    current_dict[index] = {}\n",
    "    translated_sentence1 = translate_client.translate(row['sentence1'], target_language='mk')[\"translatedText\"]\n",
    "    translated_sentence2 = translate_client.translate(row['sentence2'], target_language='mk')[\"translatedText\"]\n",
    "\n",
    "    current_dict[index]['label'] = row['label']\n",
    "    current_dict[index]['sentence1'] = translated_sentence1\n",
    "    current_dict[index]['sentence2'] = translated_sentence2\n",
    "    current_dict[index]['logic'] = row['logic']\n",
    "    current_dict[index]['predicate-argument-structure'] = row['predicate-argument-structure']\n",
    "    current_dict[index]['lexical-semantics'] = row['lexical-semantics']\n",
    "    current_dict[index]['knowledge'] = row['knowledge']\n",
    "\n",
    "    if index % 500 == 0:\n",
    "        with open(translated_dataset_dest, \"w\") as outfile:\n",
    "            json.dump(current_dict, outfile, ensure_ascii=False)\n",
    "\n",
    "with open(translated_dataset_dest, \"w\") as outfile:\n",
    "    json.dump(current_dict, outfile, ensure_ascii=False)\n",
    "\n",
    "# time.sleep(0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. AX-g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset_dest = f\"AX-g/AX-g.jsonl\"\n",
    "df_final = read_and_prepare_data(original_dataset_dest)\n",
    "translated_dataset_dest = f\"New/ax-g_mk.json\"\n",
    "current_dict = {}\n",
    "\n",
    "for index, row in tqdm(df_final.iterrows()):\n",
    "\n",
    "\n",
    "    current_dict[index] = {}\n",
    "    translated_hypothesis = translate_client.translate(row['hypothesis'], target_language='mk')[\"translatedText\"]\n",
    "    translated_premise = translate_client.translate(row['premise'], target_language='mk')[\"translatedText\"]\n",
    "\n",
    "    current_dict[index]['label'] = row['label']\n",
    "    current_dict[index]['pair_id'] = row['pair_id']\n",
    "    current_dict[index]['hypothesis'] = translated_hypothesis\n",
    "    current_dict[index]['premise'] = translated_premise\n",
    "    \n",
    "\n",
    "    if index % 500 == 0:\n",
    "        with open(translated_dataset_dest, \"w\") as outfile:\n",
    "            json.dump(current_dict, outfile, ensure_ascii=False)\n",
    "\n",
    "with open(translated_dataset_dest, \"w\") as outfile:\n",
    "    json.dump(current_dict, outfile, ensure_ascii=False)\n",
    "\n",
    "# time.sleep(0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. CB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtype in dtypes:\n",
    "    print(dtype)\n",
    "    original_dataset_dest = f\"CB/{dtype}.jsonl\"\n",
    "    translated_dataset_dest = f\"New/cb_{dtype}_mk.json\"\n",
    "    \n",
    "    df_final = read_and_prepare_data(original_dataset_dest)\n",
    "    \n",
    "    current_dict = {}\n",
    "    \n",
    "    for index, row in tqdm(df_final.iterrows()):\n",
    "\n",
    "    \n",
    "        current_dict[index] = {}\n",
    "        translated_premise = translate_client.translate(row['premise'], target_language='mk')[\"translatedText\"]\n",
    "        translated_hypothesis = translate_client.translate(row['hypothesis'], target_language='mk')[\"translatedText\"]\n",
    "\n",
    "        current_dict[index]['premise'] = translated_premise\n",
    "        current_dict[index]['hypothesis'] = translated_hypothesis\n",
    "        \n",
    "\n",
    "        if dtype != \"test\":\n",
    "            current_dict[index]['label'] = row['label']\n",
    "\n",
    "        if index % 500 == 0:\n",
    "            with open(translated_dataset_dest, \"w\") as outfile:\n",
    "                json.dump(current_dict, outfile, ensure_ascii=False)\n",
    "    \n",
    "    with open(translated_dataset_dest, \"w\") as outfile:\n",
    "        json.dump(current_dict, outfile, ensure_ascii=False)\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. WSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtype in dtypes:\n",
    "    print(dtype)\n",
    "    original_dataset_dest = f\"WSC/{dtype}.jsonl\"\n",
    "    translated_dataset_dest = f\"New/wsc_{dtype}_mk.json\"\n",
    "    \n",
    "    df_final = read_and_prepare_data(original_dataset_dest)\n",
    "    \n",
    "    current_dict = {}\n",
    "    \n",
    "    for index, row in tqdm(df_final.iterrows()):\n",
    "\n",
    "        current_dict[index] = {}\n",
    "        translated_text = translate_client.translate(row['text'], target_language='mk')[\"translatedText\"]\n",
    "        translated_span1 = translate_client.translate(row['target.span1_text'], target_language='mk')[\"translatedText\"]\n",
    "        translated_span2 = translate_client.translate(row['target.span2_text'], target_language='mk')[\"translatedText\"]\n",
    "\n",
    "        current_dict[index]['text'] = translated_text\n",
    "        current_dict[index]['target'] = {'span1_index':row['target.span1_index'], 'span2_index':row['target.span2_index'],\n",
    "                                        'span1_text':translated_span1, 'span2_text':translated_span2}\n",
    "        \n",
    "\n",
    "        if dtype != \"test\":\n",
    "            current_dict[index]['label'] = row['label']\n",
    "\n",
    "        if index % 500 == 0:\n",
    "            with open(translated_dataset_dest, \"w\",  encoding=\"utf-8\") as outfile:\n",
    "                json.dump(current_dict, outfile, ensure_ascii=False)\n",
    "    \n",
    "    with open(translated_dataset_dest, \"w\",  encoding=\"utf-8\") as outfile:\n",
    "        json.dump(current_dict, outfile, ensure_ascii=False)\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
